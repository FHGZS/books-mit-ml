回归问题
===========

引子
--------------

假定我们现有一大批数据，包含房屋的面积信息及对应该面积的房价的信息，如果我们能得到房屋面积与房屋价格间的关系，那么，当给定一个房屋时，我们只要知道其面积，就能大致推测出其价格了。

上面的问题还可以被转义如下：

> “OK，我具备了很多关于房屋面积及其对应售价的知识（数据），再通过一定的学习，当面对新的房屋面积时，我不再对其定价感到束手无策”。

通常，这类预测问题可以用**回归模型（regression）**进行解决，回归模型定义了**输入**与**输出**的关系，输入即现有知识，而输出则为预测。

一个预测问题在回归模型下的解决步骤为：

1. **积累知识**： 我们将储备的知识称之为**训练集 Training Set**，很好理解，知识能够训练人进步
2. **学习**：学习如何预测，得到输入与输出的关系。在学习阶段，应当有合适的指导方针，江山不能仅凭热血就攻下。在这里，合适的指导方针我们称之为**学习算法 Learning Algorithm**
3. **预测**：学习完成后，当接受了新的数据（输入）后，我们就能通过学习阶段获得的**对应关系**来预测输出。

学习过程往往是艰苦的，“人谁无过，过而能改，善莫大焉”，因此对我们有这两点要求：
- 有**手段**能评估我们的学习正确性
- 当学习效果不佳时，有**手段**能纠正我们的学习策略

线性回归
-----------

### 预测

首先，我们明确几个常用的数学符号：
- **特征（feature）**：$$x_i$$， 比如，房屋的面积，卧室数量都算房屋的特征
- **特征向量（输入）**：$$x$$，一套房屋的信息就算一个特征向量，特征向量由特征组成，$$x_j(i)$$ 表示第 $$i$$ 个特征向量的第 $$j$$ 个特征。
- **输出向量**：$$y$$，$$y_i$$ 表示了第 $$i$$ 个输入所对应的输出
- **假设（hypothesis）**：也称为预测函数，比如一个线性预测函数是：

$$

h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\dots+\theta_nx_n=\theta^Tx

$$

上面的表达式也称之为**回归方程（regression equation）**，$$\theta$$为回归系数。由于输入 $$x$$ 是固定不变的，所以回归系数 $$\theta$$ 将是保证我们预测准度的基石。

### 误差评估

之前我们说到，需要某个**手段**来评估我们的学习效果，即评估各个真实值 $$y_i$$ 与预测值 $$h_\theta(x^{(i)})$$ 之间的误差。最常见的，我们通过**最小均方（Least Mean Square）**来计算误差：

$$

J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^{m}[h_\theta(x^{(i)})-y_i]^2 \quad \mbox{$m$ 为样本数}

$$

该函数的矩阵表达为：

$$

J(\theta)=\frac{1}{2m}(X\theta-y)^T(X\theta-y)

$$

误差评估的函数在机器学习中也称为**代价函数（cost function）**。

### 调优：批量梯度下降

在引入了代价函数后，解决了“有手段评估学习的正确性”的问题，下面我们开始解决“当学习效果不佳时，有手段能纠正我们的学习策略”的问题。

首先可以明确的是，该手段就是要反复调节 $$\theta$$ 是的预测 $$J(\theta)$$ 足够小，以及使得预测精度足够高，在线性回归中，通常使用**梯度下降（Gradient Descent）**来调节 $$\theta$$：

$$

\theta_j = \theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta) \quad \mbox{$\alpha$ 为学习率}

$$

数学上，梯度方向是函数值下降最为剧烈的方向。那么，沿着 $$J(\theta)$$ 的梯度方向走，我们就能接近其最小值，或者极小值，亦即接近更高的预测精度。**学习率$$\alpha$$**是个相当玄乎的参数，其标识了沿梯度方向行进的速率，步子大了容易扯着蛋，很可能这一步就迈过了最小值。而步子小了，又会减缓我们找到最小值的速率。在实际编程中，学习率可以以 3 倍，10 倍这样进行取值，如：

$$

\alpha = 0.001,0.003,0.01\dots0.3,1

$$

对于一个样本容量为$$m$$的训练集，我们定义 $$\theta$$ 的调优过程为：

$$

\mbox{重复直到收敛（Repeat until convergence）：}
\\ \theta_j = \theta_j+\alpha\frac{1}{m}\sum\limits_{i=1}^{m}[y_i-h_\theta(x^{(i)})]x_j^{(i)}

$$

该函数的矩阵（向量）表达如下：

$$

\theta_j = \theta_j + \alpha\frac{1}{m}(y-X\theta)^Tx_j

$$

其中，代价函数为：

$$

J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^{m}[h_\theta(x^{(i)})-y_i]^2

$$

我们称该过程为**基于最小均方（LMS）的批量梯度下降法（Batch Gradient Descent）**，一方面，该方法虽然可以收敛（即总能达到最小值），但是每调节一个 $$\theta_j$$，都不得不迭代一遍样本集，如果样本的体积 $$m$$ 很大，这样做无疑开销巨大。另一方面，因为其可化解为向量型表示，所以就能利用到**并行计算**优化性能。

### 调优：随机梯度下降

鉴于**批量梯度下降**的性能问题，又引入了**随机梯度下降（Stochastic Gradient Descent）**：

$$

\begin{align*}
& \mbox{重复直到收敛（Repeat until convergence）:} \\
& \quad \mbox{for $i=1$ to m}: \\ 
& \quad \quad \theta_j = \theta_j+\alpha[y_i-h_\theta(x^{(i)})]x_j^{(i)}
\end{align*}

$$

可以看到，在随机梯度下降法中，每次更新 $$\theta_j$$ 只需要一个样本：$$(x^{(i)}, y_i)$$。即便在样本集容量巨大时，我们也很可能迅速获得最优解，SGD 在性能上的优势明显。

| 手段           | 概括                                 | 优点                                           | 缺点                                                               |
|:---------------|:-------------------------------------|:-----------------------------------------------|:-------------------------------------------------------------------|
| 批量梯度下降法 | 尽可能减小训练样本的**总的**预测代价 | 能够获得最优解，即最高的预测精度，支持并行计算 | 样本容量较大时，性能显著下降                                       |
| 随机梯度下降法 | 尽可能的减小每个训练样本的预测代价   | 训练速度快                                     | 并不一定能获得全局最优，经常出现抖动和噪音，且不能通过并行计算优化 |
