线性回归
=================

预测
---------

首先，我们明确几个常用的数学符号：
- **特征（feature）**：$$x_i$$， 比如，房屋的面积，卧室数量都算房屋的特征
- **特征向量（输入）**：$$x$$，一套房屋的信息就算一个特征向量，特征向量由特征组成，$$x_j^{(i)}$$ 表示第 $$i$$ 个特征向量的第 $$j$$ 个特征。
- **输出向量**：$$y$$，$$y^{(i)}$$ 表示了第 $$i$$ 个输入所对应的输出
- **假设（hypothesis）**：也称为预测函数，比如一个线性预测函数是：

$$

h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\dots+\theta_nx_n=\theta^Tx

$$

上面的表达式也称之为**回归方程（regression equation）**，$$\theta$$ 为回归系数，它是我们预测准度的基石。

误差评估
--------------

之前我们说到，需要某个**手段**来评估我们的学习效果，即评估各个真实值 $$y^{(i)}$$ 与预测值 $$h_\theta(x^{(i)})$$ 之间的差异。最常见的，我们通过**最小均方（Least Mean Square）**来描述误差：

$$

J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2,\quad \mbox{$m$ 为样本数}

$$

其矩阵表达为：

$$

J(\theta)=\frac{1}{2m}(X\theta-y)^T(X\theta-y)

$$

误差评估的函数在机器学习中也称为**代价函数（cost function）**。

批量梯度下降
-------------

在引入了代价函数后，解决了“有手段评估学习的正确性”的问题，下面我们开始解决“当学习效果不佳时，有手段能纠正我们的学习策略”的问题。

首先可以明确的是，该手段就是要反复调节 $$\theta$$ 是的预测 $$J(\theta)$$ 足够小，以及使得预测精度足够高，在线性回归中，通常使用**梯度下降（Gradient Descent）**来调节 $$\theta$$：

$$

\theta_j = \theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta) \quad \mbox{$\alpha$ 为学习率}

$$

数学上，梯度方向是函数值下降最为剧烈的方向。那么，沿着 $$J(\theta)$$ 的梯度方向走，我们就能接近其最小值，或者极小值，从而接近更高的预测精度。**学习率 $$\alpha$$ **是个相当玄乎的参数，其标识了沿梯度方向行进的速率，步子大了容易扯着蛋，很可能这一步就迈过了最小值。而步子小了，又会减缓我们找到最小值的速率。在实际编程中，学习率可以以 3 倍，10 倍这样进行取值尝试，如：

$$

\alpha = 0.001,0.003,0.01 \dots 0.3,1

$$

对于一个样本容量为 $$m$$ 的训练集，我们定义 $$\theta$$ 的调优过程为：

$$

\begin{align*}
\mbox{重复直到收敛（Repeat until convergence）：} \\
& \quad \theta_j = \theta_j+\alpha\frac{1}{m}\sum\limits_{i=1}^{m}(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}
\end{align*}

$$

该函数的矩阵（向量）表达如下：

$$

\theta_j = \theta_j + \alpha\frac{1}{m}(y-X\theta)^Tx_j

$$

其中，代价函数为：

$$

J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2

$$

我们称该过程为**基于最小均方（LMS）的批量梯度下降法（Batch Gradient Descent）**，一方面，该方法虽然可以收敛到最小值，但是每调节一个 $$\theta_j$$，都不得不遍历一遍样本集，如果样本的体积 $$m$$ 很大，这样做无疑开销巨大。但另一方面，因为其可化解为向量型表示，所以就能利用到**并行计算**优化性能。

随机梯度下降
----------------

鉴于批量梯度下降的性能问题，又引入了**随机梯度下降（Stochastic Gradient Descent）**：

$$

\begin{align*}
& \mbox{重复直到收敛（Repeat until convergence）:} \\
& \quad \mbox{for $i=1$ to $m$}: \\
& \quad \quad \theta_j = \theta_j+\alpha(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}
\end{align*}

$$

可以看到，在随机梯度下降法中，每次更新 $$\theta_j$$ 只需要一个样本：$$(x^{(i)}, y^{(i)})$$。即便在样本集容量巨大时，我们也很可能迅速获得最优解，此时 SGD 能带来明显的性能提升。

| 手段           | 概括                                 | 优点                                           | 缺点                                                               |
|:---------------|:-------------------------------------|:-----------------------------------------------|:-------------------------------------------------------------------|
| 批量梯度下降法 | 尽可能减小训练样本的**总的**预测代价 | 能够获得最优解，支持并行计算 | 样本容量较大时，性能显著下降                                       |
| 随机梯度下降法 | 尽可能的减小**每个**训练样本的预测代价   | 训练速度快                                     | 并不一定能获得全局最优，经常出现抖动和噪音，且不能通过并行计算优化 |
