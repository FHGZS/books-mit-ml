# 回归问题-程序示例
下面我们将撰写程序，其中提供了批量梯度下降和随机梯度下降两种学习策略来求解回归方程：

```python
# coding: utf-8
import numpy as np
import time

def exeTime(func):
    """ 耗时计算装饰器
    """
    def newFunc(*args, **args2):
        t0 = time.time()
        back = func(*args, **args2)
        return back, time.time() - t0
    return newFunc

def loadDataSet(filename):
    """ 读取数据

    从文件中获取数据，在《机器学习实战中》，数据格式如下
    "feature1 TAB feature2 TAB feature3 TAB label"

    Args:
        filename: 文件名

    Returns:
        X: 训练样本集矩阵
        y: 标签集矩阵
    """
    numFeat = len(open(filename).readline().split('\t')) - 1
    X = []
    y = []
    file = open(filename)
    for line in file.readlines():
        lineArr = []
        curLine = line.strip().split('\t')
        for i in range(numFeat):
            lineArr.append(float(curLine[i]))
        X.append(lineArr)
        y.append(float(curLine[-1]))
    return np.mat(X), np.mat(y).T

def h(theta, x):
    """预测函数

    Args:
        theta: 相关系数矩阵
        x: 特征向量

    Returns:
        预测结果
    """
    return (theta.T*x)[0,0]

def J(theta, X, y):
    """代价函数

    Args:
        theta: 相关系数矩阵
        X: 样本集矩阵
        y: 标签集矩阵

    Returns:
        预测误差（代价）
    """
    result = 0.0
    m = len(X)
    for i in range(m):
        diff = pow(y[i,0] - h(theta, X[i].T), 2)
        result = result + diff
    return result/(2*m)

@exeTime
def bgd(rate, maxLoop, epsilon, X, y):
    """批量梯度下降法

    Args:
        rate: 学习率
        maxLoop: 最大迭代次数
        epsilon: 收敛精度
        X: 样本矩阵
        y: 标签矩阵

    Returns:
        (theta, error, iterationCount), timeConsumed
    """
    m,n = X.shape
    # 初始化theta
    theta = np.zeros((n,1))
    count = 0
    converged = False
    error = float('inf')
    while count<=maxLoop:
        if(converged):
            break
        count = count + 1
        for j in range(n):
            if(converged):
                break
            result = 0
            for i in range(m):
                diff = pow(y[i,0] - h(theta, X[i].T),2)*X[i,j]
                result =result + diff
            theta[j,0] = theta[j,0]+rate*result/m
             # 如果已经收敛
            error = J(theta, X, y)
            if(error < epsilon):
                converged = True
    return theta,error,count-1

# 定义随机梯度下降
@exeTime
def sgd(rate, maxLoop, epsilon, X, y):
    """随机梯度下降法

    Args:
        rate: 学习率
        maxLoop: 最大迭代次数
        epsilon: 收敛精度
        X: 样本矩阵
        y: 标签矩阵

    Returns:
        (theta, error, iterationCount), timeConsumed
    """
    m,n = X.shape
    # 初始化theta
    theta = np.zeros((n,1))
    count = 0
    converged = False
    error = float('inf')
    while count <= maxLoop:
        if(converged):
            break
        count = count + 1
        for i in range(m):
            if(converged):
                break
            diff = y[i,0]-h(theta, X[i].T)
            for j in range(n):
                theta[j,0] = theta[j,0] + rate*diff*X[i, j]
            # 如果已经收敛
            error = J(theta, X, y)
            if(error < epsilon):
                converged = True
    return theta, error, count-1

```

数据来源[《机器学习实战》](https://www.manning.com/books/machine-learning-in-action)