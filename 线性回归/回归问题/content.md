# 线性回归
## 引子
先看一个问题：
假定我们现有一大批数据，包含房屋的面积信息及对应该面积的房价的信息，如果我们能得到房屋面积与房屋价格间的关系，那么，当给定一个房屋时，我们只要知道其面积，就能大致推测出其价格了。

上面的问题还可以被转义如下：
> “OK，我具备了很多关于房屋面积及其对应售价的知识（数据），再通过一定的学习，当面对新的房屋面积时，我不再对其定价感到束手无策”。

通常，这类预测问题可以用__回归模型（regression）__进行解决，回归模型定义了__输入__与__输出__的关系，输入即现有知识，而输出则为预测。

一个预测问题在回归模型下的解决步骤为：

1. __积累知识__： 我们将储备的知识称之为*训练集Training Set*，很好理解，知识能够训练人进步
2. __学习__：学习如何预测，得到输入与输出的关系。在学习阶段，应当有合适的指导方针，江山不能仅凭热血就攻下。在这里，合适的指导方针我们称之为*Learning Algorithm*
3. __预测__：学习完成后，当接受了新的数据（输入）后，我们就能通过学习阶段获得的*对应关系*来预测输出。

学习过程往往是艰苦的，“人谁无过，过而能改，善莫大焉”，因此对我们有这两点要求：
- 有__手段__能评估我们的学习正确性
- 当学习效果不佳时，有__手段__能纠正我们的学习策略

## 线性回归
### 预测
首先，我们明确几个常用的数学符号：
- __特征（feature）__：$$x_i$$， 比如，房屋的面积，卧室数量都算房屋的特征
- __特征向量（输入）__：$$x$$，一套房屋的信息就算一个特征向量，特征向量由特征组成，$$x_j(i)$$表示第$$i$$个特征向量的第$$j$$个特征。
- __输出向量__：$$y$$，$$y_i$$表示了第$$i$$个输入所对应的输出
- __假设（hypothesis）__：也称为预测函数，比如一个线性预测函数是：
$$

h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\dots+\theta_nx_n=\theta^Tx

$$
上面的表达式也称之为__回归方程（regression equation）__，$$\theta$$为回归系数。由于输入$$x$$是固定不变的，所以回归系数$$\theta$$将是保证我们预测准度的基石。

### 误差评估
之前我们说到，需要某个__手段__来评估我们的学习效果，即评估各个真实值$$y_i$$与预测值$$h_\theta(x^{(i)})$$之间的误差。最常见的，我们通过*最小均方（Least Mean Square）*来计算误差：

$$
 
J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^{m}[h_\theta(x^{(i)})-y_i]^2 \quad {m为样本数}

$$
误差评估的函数在机器学习中也称为__代价函数（cost function）__。

### 调优：批量梯度下降
在引入了代价函数后，解决了“有手段评估学习的正确性”的问题，下面我们开始解决“当学习效果不佳时，有手段能纠正我们的学习策略”的问题。

首先可以明确的是，该手段就是要反复调节$$\theta$$是的预测$$J(\theta)$$足够小，以及使得预测精度足够高，在线性回归中，通常使用*梯度下降（Gradient Descent）*来调节$$\theta$$：
$$

\theta_j = \theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta) \quad {\alpha为学习率}

$$

数学上，梯度方向是函数值下降最为剧烈的方向。那么，沿着$$J(\theta)$$的梯度方向走，我们就能接近其最小值，或者极小值，亦即接近更高的预测精度。__学习率$$\alpha$$__是个相当玄乎的参数，其标识了沿梯度方向行进的速率，步子大了容易扯着蛋，很可能这一步就迈过了最小值。而步子小了，又会减缓我们找到最小值的速率。在实际编程中，学习率可以以3倍，10倍这样进行取值，如：
$$

\alpha = 0.001,0.003,0.01\dots0.3,1

$$
对于一个样本容量为$$m$$的训练集，我们定义$$\theta$$的调优过程为：
$$

\mbox{重复直到收敛（Repeat until convergence）:}
\\ \theta_j = \theta_j+\alpha\frac{1}{m}\sum\limits_{i=1}^{m}[y_i-h_\theta(x^{(i)})]x_j^{(i)}
$$
其中，代价函数为：
$$
 
J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^{m}[h_\theta(x^{(i)})-y_i]^2

$$
我们称该过程为__基于最小均方（LMS）的批量梯度下降法（Batch Gradient Descent）__，该方法虽然可以收敛（即总能达到最小值），但是每调节一个$$\theta_j$$，都不得不迭代一遍样本集，如果样本的体积$$m$$很大，这样做无疑开销巨大。

### 调优：随机梯度下降
鉴于__批量梯度下降__的性能问题，又引入了__随机梯度下降（Stochastic Gradient Descent）__：
$$

\mbox{重复直到收敛（Repeat until convergence）:}
\\ \mbox{for }i=1 \mbox{ to } m:
\\ \theta_j = \theta_j+\alpha[y_i-h_\theta(x^{(i)})]x_j^{(i)}
$$

可以看到，在随机梯度下降法中，每次更新$$\theta_j$$只需要一个样本：$$(x^{(i)}, y_i)$$。即便在样本集容量巨大时，我们也很可能迅速获得最优解，SGD在性能上的优势明显。

|手段|特点|
|----|----|
|批量梯度下降法|尽可能减小训练样本的__总的__预测代价，能够获得最小值，即最高的预测精度|
|随机梯度下降法|尽可能的减小每个训练样本的预测代价，虽然并非从全局着眼，但是这种每个样本的最小化过程通常也能使得最终的预测精度足够高|

### 程序
下面我们拥有