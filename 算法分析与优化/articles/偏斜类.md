偏斜类（Skewed Classes）
=============

引子
---------

假定我们通过逻辑回归来预测病人是否患有癌症：

$$

y =
\begin{cases}
0, & \mbox{病人未患癌症} \\
1, & \mbox{病人患有癌症}
\end{cases}

$$

并且，令人欣喜的是，测试集的错误率只有 **1%**。别着急高兴，假如我们的测试样本中只有 **0.5%** 患有癌症，那么我们何不直接让预测函数为：

$$

h_{\theta}(x) = 0

$$

即，我们永远预测病人不患病，那么准确率会高达 **95%**。但这可不是一件好事儿，我们追求高准确率牺牲的是病人的利益。引起这个问题的原因是样本中出现了**偏斜类（Skewed Classes）**，偏斜即倾斜，大量样本倾斜向了某一类型。

查准率（Precision）与召回率（Recall）
----------------

从上面的例子我们知道，单纯地使用误差（Error）并不能完善地评价模型好坏，现在引入两个重要的评价指标：（1）查准率（Precision）；（2）召回率（Recall），并定义：

- 阳性（Positive）：表示正样本。当预测和实际都为正样本时，表示真阳性（True Positive）；如果预测为正样本，而实际为负样本，则表示假阳性（False Positive）。
- 阴性（Negative）：表示负样本。当预测和实际都为负样本时，表示真阴性（True Negative）；如果预测为负样本，而实际为正样本，则表示假阴性（False Negative）。

|       | 真值1          | 真值0          |
|:------|:---------------|:---------------|
| 预测1 | True Positive  | False Positive |
| 预测0 | False Negative | True Negative  |

则：

- **查准率（Precision）**：

$$

Precision = \frac{TruePos}{PredicatedPos} = \frac{TruePos}{TruePos+FalsePos}

$$

在上例中，查准率就描述了：在我们预测患癌的病人中，确实患了癌症的病人的比例。从公式中我们也可以得出，要想得到提高查准率，我们就要降低假阳性的出现的频次，即，我们只有在拥有十足的把握是，才预测一个样本为正样本。

- **召回率（Recall）**：
$$

Recall = \frac{TruePos}{ActualPos} = \frac{TruePos}{TruePos+FalseNeg}

$$


在上例中，召回率就描述了：在患癌的病人中，有多少病人被我们预测到了。从公式中我们也可以得出，要想提高召回率，我们就要降低假阴性出现的频次，即，尽可能不放过任何可能为正样本的样本。

查准率和召回率的权衡
--------------------

理想状况下，我们希望假设函数能够同时具备高准确率（High Precision）及高召回率（High Recall）。但是往往鱼和熊掌不可兼得。回到预测病人患癌的例子中，假定我们的预测函数为：

$$

y =
\begin{cases}
1, & h_{\theta}(x) \ge 0.5 \\
0, & \mbox{otherwise}
\end{cases}

$$

即，我们设定的预测阈值为 $$0.5$$。这么做似乎风险不小，很多没有患癌的病人被我们认为患有癌症而接受了不必要的治疗，因此，我们调高我们的阈值为 $$0.7$$：

$$

y =
\begin{cases}
1, & h_\theta(x) \ge 0.7 \\
0, & \mbox{otherwise}
\end{cases}

$$

此时，必须有较高的把握，我们才会预测一个患有癌症，避免非癌症患者接受到了不必要的治疗，假阳性样本少，此时我们也获得了**高查准率**。然而，这么预测的代价是，有些癌症病患体征不明显，就被我们认为没有患癌，从而得不到治疗，假阴性样本多，即我们的**召回率**偏低。

当我们尝试构建了不同的算法模型，并且获得了不同的查准率和召回率：

|        | Precision | Recall |
|:-------|:----------|:-------|
| 算法 1 | 0.5       | 0.4    |
| 算法 2 | 0.7       | 0.1    |
| 算法 3 | 0.02      | 1.0    |

那么选择哪个算法是最好的呢，假定我们使用平均值来权衡查准率和召回率：

$$

Average = \frac{P+R}{2}

$$

|        | Precision | Recall | Average |
|:-------|:----------|:-------|:--------|
| 算法 1 | 0.5       | 0.4    | 0.45    |
| 算法 2 | 0.7       | 0.1    | 0.4     |
| 算法 3 | 0.02      | 1.0    | 0.51    |

按照平均值，我们会选择算法 3，但是这并不是一个好的算法，因为其查准率太低了，我们希望有一个指标能选出查准率和召回率都高的算法，为此，引入了 $$F_1Score$$：

$$

F_1 Score = 2\frac{PR}{P+R}

$$

从公式中也可以看到，分子是查准率和召回率的乘积，只有二者都较高时，$$F_1Score$$ 才会较高，特别地：

$$

\begin{align*}
F_1 Score &= 0, \quad \mbox{if $P=0$ or $R=0$} \\
F_1 Score &= 1, \quad \mbox{if $P=1$ and $R=1$}
\end{align*}

$$

|        | Precision | Recall | $$F_1Score$$ |
|:-------|:----------|:-------|:-------------|
| 算法 1 | 0.5       | 0.4    | 0.444        |
| 算法 2 | 0.7       | 0.1    | 0.175        |
| 算法 3 | 0.02      | 1.0    | 0.0392       |

$$F_1Score$$ 帮我们选出了算法1，事实也确实如此，算法1的查准率和召回率都较高。
